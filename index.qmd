---
title: "Using Randomization & Stratification to Overcome A Common Cause Confounder"
subtitle: "A Case Study in Causal Inference"
format:
  html: default
bibliography: references.bib
execute:
  echo: true
  eval: true
python:
  reticulate: false
---


## The Paradox: Who *Really* Is the Better Surgeon?

::: {.callout-tip title="A Tale of Two Surgeons" icon=false}
*Paraphrased from* [@taleb2017surgeons]

<div style="float: right; margin-left: 15px; margin-bottom: 10px;">
<img src="docSideBySide.jpg" alt="Doc Dreamy and Doc Duck side by side" style="max-width: 250px; height: auto; border-radius: 5px;">
</div>

Imagine you need to choose between two surgeons of similar rank at the same hospital. The first surgeon, Doc Dreamy, matches our stereotype perfectly: refined appearance, silver-rimmed glasses, delicate hands, measured speech, and an office adorned with Ivy League diplomas (see the image). The second surgeon, Doc Duck, by contrast, looks more like a butcher—overweight, with large hands, an unkempt appearance, and no visible credentials on the wall.

Counterintuitively, the surgeon who doesn't "look the part" may actually be the better choice. Why? Because when someone succeeds in their profession despite not fitting the expected appearance, it suggests they had to overcome significant perceptual biases. And if we are lucky enough to have people who do not look the part, it is thanks to the presence of some skin in the game, the contact with reality that filters out incompetence. [@taleb2017surgeons]
:::

## Observational Data: A Misleading Victory for Doc Dreamy

```{python}
#| label: simulate-data
#| echo: true
#| message: false
#| warning: false
#| include: false
#| eval: false

import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(123)

# Create dataframe with 100 patients and severity scores
n_patients = 100
patients_df = (
    pd.DataFrame({
        'patient': range(1, n_patients + 1),
        'severity': np.random.normal(0, 1, n_patients)
    })
    .assign(
        # Assign doctors based on severity using sigmoid function:
        # Probability of Doc Duck is inversely proportional to severity
        # Higher severity → higher probability of Doc Duck
        # Uses sigmoid: P(Doc Duck) = 1 / (1 + exp(-k * severity))
        # where k controls the steepness of the relationship
        prob_duck=lambda df: 1 / (1 + np.exp(-1.5 * df['severity'])),  # k=1.5 controls sensitivity
        # Random assignment: if random < prob_duck → Doc Duck (id=0), else Doc Dreamy (id=1)
        doctor_id=lambda df: (np.random.random(n_patients) >= df['prob_duck']).astype(int),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.2, 1), Doc Dreamy: Normal(-0.2, 1)
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients),
            np.random.normal(-0.4, 1, n_patients)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding prob_duck and surgicalGoodness columns
patients_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data.csv', index=False
)

# Display first few rows
patients_df.head()
```

::: {.panel-tabset}
## Python

```{python}
#| label: load-data-python
#| echo: true

import pandas as pd

# Load the observational data
patients_df = pd.read_csv('patients_data.csv')

# Display first few rows to understand the structure
print(f"Number of patients: {len(patients_df)}")
patients_df.head()
```

## R

```{r}
#| label: load-data-r
#| echo: true

# Load the observational data
patients_df <- read.csv('patients_data.csv')

# Display first few rows to understand the structure
cat("Number of patients:", nrow(patients_df), "\n")
head(patients_df)
```
:::

So I started by looking at the data to see which surgeon actually performs better. Looking at @fig-plot-outcomes, I plotted the post-surgical symptom scores for all 100 patients. What I found was pretty clear: Doc Dreamy's average score was 2.8, while Doc Duck's was 3.38. Since lower scores mean better outcomes, this suggests Doc Dreamy is the better surgeon. I even ran a t-test to check if this difference was statistically significant, and it was (t = -2.317, p = 0.023). So at first glance, the data seems to show that Doc Dreamy is definitely better. 

### Figure 1: Post-Surgical Outcomes by Patient

::: {.panel-tabset}
## Python

```{python}
#| label: fig-plot-outcomes
#| fig-cap: "Post-Surgical Symptom Score (lower is better) - Observed Data"
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Calculate means for each doctor
mean_dreamy = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']['post_surgical_score'].mean()
mean_duck = patients_df[patients_df['doctor_name'] == 'Doc Duck']['post_surgical_score'].mean()

# Create figure
fig, ax = plt.subplots(figsize=(7, 4))

# Separate data by doctor
dreamy_data = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']
duck_data = patients_df[patients_df['doctor_name'] == 'Doc Duck']

# Plot scatter points
ax.scatter(dreamy_data['patient'], dreamy_data['post_surgical_score'], 
           marker='o', color='#4E79A7', s=60, alpha=0.7, label='Doc Dreamy', zorder=3)
ax.scatter(duck_data['patient'], duck_data['post_surgical_score'], 
           marker='^', color='#E15759', s=60, alpha=0.7, label='Doc Duck', zorder=3)

# Add horizontal mean lines
ax.axhline(y=mean_dreamy, color='#4E79A7', linestyle='--', linewidth=2, alpha=0.8, zorder=2)
ax.axhline(y=mean_duck, color='#E15759', linestyle='--', linewidth=2, alpha=0.8, zorder=2)

# Add text annotations with mean values and shape symbols
ax.text(len(patients_df) * 0.02, mean_dreamy + 0.1, f'○ {mean_dreamy:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold', va='bottom')
ax.text(len(patients_df) * 0.02, mean_duck + 0.1, f'△ {mean_duck:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold', va='bottom')

# Styling
ax.set_xlabel('Patient Number', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score by Patient and Doctor', fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=10, framealpha=0.9)
ax.set_xlim(-2, len(patients_df) + 2)

plt.tight_layout()
plt.show()
```

## R

```{r}
#| label: fig-plot-outcomes-r
#| fig-cap: "Post-Surgical Symptom Score (lower is better) - Observed Data"
#| echo: false

library(ggplot2)

# Calculate means for each doctor
mean_dreamy <- mean(patients_df$post_surgical_score[patients_df$doctor_name == 'Doc Dreamy'])
mean_duck <- mean(patients_df$post_surgical_score[patients_df$doctor_name == 'Doc Duck'])

# Create the plot
ggplot(patients_df, aes(x = patient, y = post_surgical_score, color = doctor_name, shape = doctor_name)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(aes(yintercept = mean_dreamy, color = 'Doc Dreamy'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  geom_hline(aes(yintercept = mean_duck, color = 'Doc Duck'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  annotate('text', x = max(patients_df$patient) * 0.02, y = mean_dreamy + 0.1, 
           label = paste('○', round(mean_dreamy, 2)), color = '#4E79A7', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  annotate('text', x = max(patients_df$patient) * 0.02, y = mean_duck + 0.1, 
           label = paste('△', round(mean_duck, 2)), color = '#E15759', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759')) +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17)) +
  labs(x = 'Patient Number', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score by Patient and Doctor',
       color = 'Doctor', shape = 'Doctor') +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
        axis.title = element_text(size = 12, face = 'bold'),
        axis.text = element_text(size = 10),
        legend.position = 'right',
        legend.title = element_text(face = 'bold'),
        panel.grid = element_line(alpha = 0.3))

```
:::

## The Hidden Confounder: Patient Severity Explains It All

When I first looked at @fig-plot-outcomes, my brain wanted to jump to the simplest conclusion - the one shown in @fig-example-node.

![DAG Model Explaining Surgical Outcomes](dag-example-node.png){#fig-example-node width=50%}

It seems obvious: **Doc Dreamy must be the better surgeon because his patients have lower scores**. That's what the data shows, right?

But then I learned about something called a "common cause confounder" that can trick you. Basically, sometimes two things look related, but they're not actually causing each other - they're both being caused by something else. Like, if you see puddles on the road and people with umbrellas, you might think puddles cause umbrellas. But really, rain causes both - the puddles AND the umbrellas. So there's a spurious correlation that isn't actually causal.

This made me think: what if there's a hidden factor affecting both surgeon choice AND surgical outcomes? Looking at @fig-example2-node, I started wondering if patient severity might be that hidden factor.

![DAG Model Explaining Surgical Outcomes](dag-example2-node.png){#fig-example2-node width=50%}

Here's what I think might be happening: Doc Dreamy books surgeries 3 weeks out, while Doc Duck can see patients in just 1 week. So patients who aren't in a rush (usually the less severe cases) can wait and choose Doc Dreamy based on his nice website and professional photo. But patients who are really sick and need surgery ASAP (the more severe cases) don't have time to wait - they go with Doc Duck because he's available right away. This means Doc Dreamy gets easier cases, and Doc Duck gets the harder ones.

::: {.callout-warning title="Assumption: Slow Progression of Patient Severity" icon=false}
For simplicity, we assume that patient severity progresses slowly enough that a 2-week delay (i.e., waiting for Dr. Dreamy's availability) has zero effect on surgical outcomes. This delay affects only how quickly patients receive surgical relief, not the eventual outcome itself. While time-to-surgery can be an important factor in other contexts, here we focus solely on whether initial patient severity might create a spurious or biased association between surgeon choice and outcomes.
:::

## Solution 1: Randomization — Break the Confounding Path

The best way to figure out if something actually causes an outcome is through a randomized controlled trial. Instead of letting patients pick their surgeon, what if we just randomly assigned them? That way, patient severity wouldn't influence which surgeon they get - it would be completely random.

This is exactly what randomization is for. By randomly assigning patients, we break the connection between patient severity and surgeon choice. Now it's not about who can wait or who needs surgery fast - it's just random chance. This is shown in @fig-example3-node.

![DAG Model Explaining Surgical Outcomes](dag-example3-node.png){#fig-example3-node width=65%}

In this randomized scenario, patient severity no longer determines which surgeon you get. Instead, randomization does. This breaks the confounding relationship, so we can actually see if the surgeon themselves matter, or if it was all about patient severity.

### But wait - how do we know what would have happened?

::: {.callout-note title="What is a counterfactual?" icon=false}
A counterfactual is basically asking "what if?" - like, what would have happened to each patient if they got the OTHER surgeon instead? We can never actually know this because each patient only got one surgeon. It's like asking what would have happened in a parallel universe.
:::

The problem is we can't go back in time and reassign those original patients. They already went to whoever they chose (or whoever was available). So to test what happens with randomization, we need to run a new experiment. I randomly assigned 100 new patients to either Doc Dreamy or Doc Duck, completely ignoring their severity. Here's what happened:

```{python}
#| label: simulate-data-randomized
#| echo: true
#| message: false
#| warning: false
#| include: false

import pandas as pd
import numpy as np

# Set seed for reproducibility (different from observational data)
np.random.seed(456)

# Create dataframe with 100 patients and severity scores
n_patients_randomized = 100
patients_randomized_df = (
    pd.DataFrame({
        'patient': range(1, n_patients_randomized + 1),
        'severity': np.random.normal(0, 1, n_patients_randomized)
    })
    .assign(
        # Randomly assign doctors: 50/50 chance, independent of severity
        # This breaks the confounding relationship
        doctor_id=lambda df: np.random.choice([0, 1], size=n_patients_randomized),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.5, 1), Doc Dreamy: Normal(-0.4, 1)
        # Same true effect as before - Doc Duck is better
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients_randomized),
            np.random.normal(-0.4, 1, n_patients_randomized)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding surgicalGoodness column
patients_randomized_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data_randomized.csv', index=False
)

# Display first few rows
patients_randomized_df.head()
```

::: {.panel-tabset}
## Python

```{python}
#| label: load-data-randomized-python
#| echo: true

import pandas as pd

# Load the randomized data
patients_randomized_df = pd.read_csv('patients_data_randomized.csv')

# Display first few rows
print(f"Number of randomized patients: {len(patients_randomized_df)}")
patients_randomized_df.head()
```

## R

```{r}
#| label: load-data-randomized-r
#| echo: true

# Load the randomized data
patients_randomized_df <- read.csv('patients_data_randomized.csv')

# Display first few rows
cat("Number of randomized patients:", nrow(patients_randomized_df), "\n")
head(patients_randomized_df)
```
:::

Looking at @fig-plot-outcomes-randomized, the results are completely different! With random assignment, Doc Duck's average score is 2.71 and Doc Dreamy's is 3.46. Since lower is better, Doc Duck is actually performing better. I ran another t-test and the difference is statistically significant (t = 2.734, p = 0.007). This is crazy - **not only is Doc Dreamy NOT the better surgeon, he's actually worse!** The original data totally misled me because of the confounding.

### Figure 2: Randomized Assignment Outcomes

::: {.panel-tabset}
## Python

```{python}
#| label: fig-plot-outcomes-randomized
#| fig-cap: "Post-Surgical Symptom Score (lower is better) for randomized patients."
#| echo: false

# Calculate means for each doctor in randomized data
mean_dreamy_rand = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Dreamy']['post_surgical_score'].mean()
mean_duck_rand = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Duck']['post_surgical_score'].mean()

# Create figure
fig, ax = plt.subplots(figsize=(7, 4))

# Separate data by doctor
dreamy_data_rand = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Dreamy']
duck_data_rand = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Duck']

# Plot scatter points
ax.scatter(dreamy_data_rand['patient'], dreamy_data_rand['post_surgical_score'], 
           marker='o', color='#4E79A7', s=60, alpha=0.7, label='Doc Dreamy', zorder=3)
ax.scatter(duck_data_rand['patient'], duck_data_rand['post_surgical_score'], 
           marker='^', color='#E15759', s=60, alpha=0.7, label='Doc Duck', zorder=3)

# Add horizontal mean lines
ax.axhline(y=mean_dreamy_rand, color='#4E79A7', linestyle='--', linewidth=2, alpha=0.8, zorder=2)
ax.axhline(y=mean_duck_rand, color='#E15759', linestyle='--', linewidth=2, alpha=0.8, zorder=2)

# Add text annotations with mean values and shape symbols
ax.text(len(patients_randomized_df) * 0.02, mean_dreamy_rand + 0.1, f'○ {mean_dreamy_rand:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold', va='bottom')
ax.text(len(patients_randomized_df) * 0.02, mean_duck_rand + 0.1, f'△ {mean_duck_rand:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold', va='bottom')

# Styling
ax.set_xlabel('Patient Number', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score (Randomized Assignment)', fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=10, framealpha=0.9)
ax.set_xlim(-2, len(patients_randomized_df) + 2)

plt.tight_layout()
plt.show()
```

## R

```{r}
#| label: fig-plot-outcomes-randomized-r
#| fig-cap: "Post-Surgical Symptom Score (lower is better) for randomized patients."
#| echo: false

# Calculate means for each doctor in randomized data
mean_dreamy_rand <- mean(patients_randomized_df$post_surgical_score[patients_randomized_df$doctor_name == 'Doc Dreamy'])
mean_duck_rand <- mean(patients_randomized_df$post_surgical_score[patients_randomized_df$doctor_name == 'Doc Duck'])

# Create the plot
ggplot(patients_randomized_df, aes(x = patient, y = post_surgical_score, color = doctor_name, shape = doctor_name)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(aes(yintercept = mean_dreamy_rand, color = 'Doc Dreamy'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  geom_hline(aes(yintercept = mean_duck_rand, color = 'Doc Duck'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  annotate('text', x = max(patients_randomized_df$patient) * 0.02, y = mean_dreamy_rand + 0.1, 
           label = paste('○', round(mean_dreamy_rand, 2)), color = '#4E79A7', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  annotate('text', x = max(patients_randomized_df$patient) * 0.02, y = mean_duck_rand + 0.1, 
           label = paste('△', round(mean_duck_rand, 2)), color = '#E15759', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759')) +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17)) +
  labs(x = 'Patient Number', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score (Randomized Assignment)',
       color = 'Doctor', shape = 'Doctor') +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
        axis.title = element_text(size = 12, face = 'bold'),
        axis.text = element_text(size = 10),
        legend.position = 'right',
        legend.title = element_text(face = 'bold'),
        panel.grid = element_line(alpha = 0.3))

```
:::

## Solution 2: Stratification — When You Can't Randomize

But what if you can't randomize? Like, you already have the data and you can't go back and reassign people. That's where stratification comes in. The idea is to look at patients who are similar in terms of the confounder (severity), and then compare surgeons within those groups. So instead of comparing all of Doc Dreamy's patients to all of Doc Duck's patients, we compare them only within similar severity groups.

I made a plot showing patient severity on the x-axis and post-surgical scores on the y-axis. This lets us see what happens when we compare like to like.  

### Figure 3: Outcomes by Patient Severity (Stratification)

::: {.panel-tabset}
## Python

```{python}
#| label: fig-plot-outcomes-severity
#| fig-cap: "Post-Surgical Symptom Score (lower is better) by Patient and Severity"
#| echo: false

# Calculate means for each doctor (using observational data)
mean_dreamy_sev = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']['post_surgical_score'].mean()
mean_duck_sev = patients_df[patients_df['doctor_name'] == 'Doc Duck']['post_surgical_score'].mean()

# Create figure
fig, ax = plt.subplots(figsize=(7, 4))

# Add shaded region from -1 to 1 on x-axis (highlighted region for stratification)
ax.axvspan(-1, 1, alpha=0.15, color='gray', zorder=0, label='Overlap Region')

# Separate data by doctor
dreamy_data_sev = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']
duck_data_sev = patients_df[patients_df['doctor_name'] == 'Doc Duck']

# Plot scatter points
ax.scatter(dreamy_data_sev['severity'], dreamy_data_sev['post_surgical_score'], 
           marker='o', color='#4E79A7', s=60, alpha=0.7, label='Doc Dreamy', zorder=3)
ax.scatter(duck_data_sev['severity'], duck_data_sev['post_surgical_score'], 
           marker='^', color='#E15759', s=60, alpha=0.7, label='Doc Duck', zorder=3)

# Add horizontal mean lines
ax.axhline(y=mean_dreamy_sev, color='#4E79A7', linestyle='--', linewidth=2, alpha=0.8, zorder=2)
ax.axhline(y=mean_duck_sev, color='#E15759', linestyle='--', linewidth=2, alpha=0.8, zorder=2)

# Get x-axis limits for annotation positioning
x_min, x_max = patients_df['severity'].min(), patients_df['severity'].max()
x_range = x_max - x_min

# Add text annotations with mean values and shape symbols
# Position annotations near the left side of the plot
ax.text(x_min + 0.05 * x_range, mean_dreamy_sev + 0.1, 
        f'○ {mean_dreamy_sev:.2f}', color='#4E79A7', fontsize=11, fontweight='bold', va='bottom')
ax.text(x_min + 0.05 * x_range, mean_duck_sev + 0.1, 
        f'△ {mean_duck_sev:.2f}', color='#E15759', fontsize=11, fontweight='bold', va='bottom')

# Styling
ax.set_xlabel('Initial Patient Severity', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score by Patient Severity', fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=10, framealpha=0.9)

plt.tight_layout()
plt.show()
```

## R

```{r}
#| label: fig-plot-outcomes-severity-r
#| fig-cap: "Post-Surgical Symptom Score (lower is better) by Patient and Severity"
#| echo: false

# Calculate means for each doctor (using observational data)
mean_dreamy_sev <- mean(patients_df$post_surgical_score[patients_df$doctor_name == 'Doc Dreamy'])
mean_duck_sev <- mean(patients_df$post_surgical_score[patients_df$doctor_name == 'Doc Duck'])

# Get x-axis range for annotation positioning
x_min <- min(patients_df$severity)
x_max <- max(patients_df$severity)
x_range <- x_max - x_min

# Create the plot
ggplot(patients_df, aes(x = severity, y = post_surgical_score, color = doctor_name, shape = doctor_name)) +
  annotate('rect', xmin = -1, xmax = 1, ymin = -Inf, ymax = Inf, 
           alpha = 0.15, fill = 'gray', color = NA) +
  geom_point(size = 3, alpha = 0.7) +
  geom_hline(aes(yintercept = mean_dreamy_sev, color = 'Doc Dreamy'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  geom_hline(aes(yintercept = mean_duck_sev, color = 'Doc Duck'), 
             linetype = 'dashed', linewidth = 1, alpha = 0.8, show.legend = FALSE) +
  annotate('text', x = x_min + 0.05 * x_range, y = mean_dreamy_sev + 0.1, 
           label = paste('○', round(mean_dreamy_sev, 2)), color = '#4E79A7', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  annotate('text', x = x_min + 0.05 * x_range, y = mean_duck_sev + 0.1, 
           label = paste('△', round(mean_duck_sev, 2)), color = '#E15759', 
           fontface = 'bold', size = 3.5, hjust = 0) +
  scale_color_manual(values = c('Doc Dreamy' = '#4E79A7', 'Doc Duck' = '#E15759')) +
  scale_shape_manual(values = c('Doc Dreamy' = 19, 'Doc Duck' = 17)) +
  labs(x = 'Initial Patient Severity', 
       y = 'Post-Surgical Symptom Score (Lower is Better)',
       title = 'Post-Surgical Symptom Score by Patient Severity',
       color = 'Doctor', shape = 'Doctor') +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = 'bold', hjust = 0.5, margin = margin(b = 15)),
        axis.title = element_text(size = 12, face = 'bold'),
        axis.text = element_text(size = 10),
        legend.position = 'right',
        legend.title = element_text(face = 'bold'),
        panel.grid = element_line(alpha = 0.3))

```
:::

Looking at @fig-plot-outcomes-severity, I focused on the region between -1 and 1 on the severity scale (the gray shaded area). I picked this because both surgeons have a lot of patients in this range, so we can actually compare them fairly. Outside this range, there's not much overlap - Doc Duck has way more high-severity patients (to the right), and Doc Dreamy has more low-severity ones (to the left).

But here's the key: within that highlighted region where we can compare them fairly, Doc Dreamy's points (blue circles) are actually higher (worse) than Doc Duck's points (red triangles). You can also see that overall, Doc Duck's patients are shifted to the right on the severity axis - meaning he's getting the sicker patients. This is exactly what you'd expect if severity is causing both surgeon choice AND outcomes.

So even though the overall average makes Doc Dreamy look better, when I compare them on similar patients, Doc Duck actually does better. The original conclusion was totally wrong because I wasn't accounting for the severity confounder.

## What I Learned: Understanding Confounding

This whole thing really opened my eyes to how easy it is to be fooled by data. The numbers don't lie, but they might be answering a different question than you think.

At first, the raw data made it look like Doc Dreamy was clearly better. His average scores were lower, which is good. But here's the thing - correlation doesn't mean causation. I know we all hear that, but I never really understood it until now. Our brains want to see patterns and find causes, so we jump to conclusions.

The real problem here is patient severity. It's like this hidden puppeteer pulling strings behind the scenes. It makes Doc Dreamy look amazing by giving him all the easy cases, while Doc Duck gets stuck with the really sick patients. Without randomization, there's no way to tell if Doc Dreamy is actually better, or if he just got lucky with easier patients.

Randomization fixes this. When you randomly assign patients, severity can't influence who gets which surgeon. It's like flipping a coin - completely random. And when I did that, the truth came out: Doc Duck is actually the better surgeon. The data wasn't wrong before, it was just answering "who has better outcomes given their patient assignments" instead of "who is the better surgeon."

But most of the time, you can't randomize. You already have the data. That's when you use stratification - you compare surgeons only within groups of similar patients. It's like comparing apples to apples instead of apples to oranges. It's not as clean as randomization, but it's way better than being completely wrong.

## Conclusion

So here's what I learned from this whole analysis: **the differences in outcomes are mostly because of patient severity, not because of which surgeon is better**. When I first looked at the data, it seemed obvious that Doc Dreamy was better - his patients had lower scores. But I was totally wrong because I didn't account for the confounding.

Once I randomized the assignments, the real story came out. Doc Duck's patients actually had better outcomes (average 2.71 vs. 3.46). The original data was completely backwards!

When I stratified by severity and compared surgeons on similar patients, the same pattern showed up - Doc Duck does better. The only reason Doc Dreamy looked good in the original data is because he got all the easy cases, while Doc Duck got stuck with the really sick patients.

**Bottom line: you can't tell if the surgeon actually matters without dealing with the confounding.** If you don't randomize or stratify, you have no idea if the differences you see are because of the surgeon's skill or just because of differences in the patients they're treating. This is why randomized controlled trials are so important - they're the gold standard. But when you can't randomize, stratification is your next best option.

The main thing I'm taking away from this is that data can be really misleading if you don't think carefully about what might be confounding your results. Drawing a DAG and thinking about what causes what helps a lot. Randomize when you can, stratify when you can't. The methods aren't that complicated - the hard part is remembering to actually use them instead of just trusting the first thing you see in the data.

